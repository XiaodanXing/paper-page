<html>
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->
  <style type="text/css">
   @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  </style>
  <title>
   CS²: A Controllable and Simultaneous Synthesizer of Images and Annotations with Minimal Human Intervention
  </title>
 </head>
 <body>
  <br/>
  <center>
   <span style="font-size:36px">
    CS²: A Controllable and Simultaneous Synthesizer of Images and Annotations with Minimal Human Intervention
   </span>
   <br/>
   <br/>
   <br/>
  </center>
  <table align="center" id="authors_new" width="800px">
   <tbody>
    <tr>
     <td align="center" width="160px">
      <center>
       <span style="font-size:16px">
        Xiaodan Xing
        <sup>
         1
        </sup>
       </span>
      </center>
     </td>
     <td align="center" width="160px">
      <center>
       <span style="font-size:16px">
        Jiahao Huang
        <sup>
         1,5
        </sup>
       </span>
      </center>
     </td>
     <td align="center" width="160px">
      <center>
       <span style="font-size:16px">
        Yang Nan
        <sup>
         1
        </sup>
       </span>
      </center>
     </td>
     <td align="center" width="160px">
      <center>
       <span style="font-size:16px">
        Yinzhe Wu
        <sup>
         1,2
        </sup>
       </span>
      </center>
     </td>
     <td align="center" width="160px">
      <center>
       <span style="font-size:16px">
        Chenjia Wang
        <sup>
         3
        </sup>
       </span>
      </center>
     </td>
    </tr>
    <tr>
     <td align="center" width="160px">
      <center>
       <span style="font-size:16px">
        Zhifan Gao
        <sup>
         4
        </sup>
       </span>
      </center>
     </td>
     <td align="center" width="160px">
      <center>
       <span style="font-size:16px">
        Simon Walsh
        <sup>
         1
        </sup>
       </span>
      </center>
     </td>
     <td align="center" width="160px">
      <center>
       <span style="font-size:16px">
        Guang Yang
        <sup>
         1,5
        </sup>
       </span>
      </center>
     </td>
    </tr>
   </tbody>
  </table>
  <br/>
  <br/>
  <table align="center" id="instutions_new" width="800px">
   <tbody>
    <tr align="center" width="160px">
     <center>
      <span style="font-size:16px">
       <sup>
        1
       </sup>
       National Heart and Lung Institute, Imperial College London, London, UK
      </span>
     </center>
    </tr>
    <tr align="center" width="160px">
     <center>
      <span style="font-size:16px">
       <sup>
        2
       </sup>
       Department of Biomedical and Engineering, Imperial College London, London, UK
      </span>
     </center>
    </tr>
    <tr align="center" width="160px">
     <center>
      <span style="font-size:16px">
       <sup>
        3
       </sup>
       Edinburgh Centre for Robotics, Heriot-Watt University, Edinburgh, UK
      </span>
     </center>
    </tr>
    <tr align="center" width="160px">
     <center>
      <span style="font-size:16px">
       <sup>
        4
       </sup>
       School of Biomedical Engineering, Sun Yat-sen University, Guangdong, China
      </span>
     </center>
    </tr>
    <tr align="center" width="160px">
     <center>
      <span style="font-size:16px">
       <sup>
        5
       </sup>
       Cardiovascular Research Centre, Royal Brompton Hospital, London, UK
      </span>
     </center>
    </tr>
   </tbody>
  </table>
  <br/>
  <table align="center" id="links" width="700px">
   <tbody>
    <tr>
     <td align="center" width="200px">
      <center>
       <br/>
       <span style="font-size:20px">
        Code
        <a href="https://github.com/ayanglab/CS2">
         [GitHub]
        </a>
       </span>
      </center>
     </td>
     <td align="center" width="200px">
      <center>
       <br/>
       <span style="font-size:20px">
        Paper
        <a href="https://github.com/ayanglab/CS2">
         [arXiv]
        </a>
       </span>
      </center>
     </td>
     <td align="center" width="200px">
      <center>
       <br/>
       <span style="font-size:20px">
        Cite
        <a href="./resources/cite.txt">
         [BibTeX]
        </a>
       </span>
      </center>
     </td>
    </tr>
   </tbody>
  </table>
  <br/>
  <hr/>
  <center>
   <h2>
    Abstract
   </h2>
  </center>
  <p id="abstract" style="text-align:justify; text-justify:inter-ideograph;">
   <left>
    The destitution of image data and corresponding expert annotations limit the training capacities of AI diagnostic models and potentially inhibit their performance. To address such a problem of data and label scarcity, generative models have been developed to augment the training datasets. Previously proposed generative models usually require manually adjusted annotations (e.g., segmentation masks) or need pre-labeling. However, studies have found that these pre-labeling based methods can induce hallucinating artifacts, which might mislead the downstream clinical tasks, while manual adjustment could be onerous and subjective. To avoid manual adjustment and pre-labeling, we propose a novel controllable and simultaneous synthesizer (dubbed CS²) in this study to generate both realistic images and corresponding annotations at the same time. Our CS² model is trained and validated using high resolution CT (HRCT) data collected from COVID-19 patients to realize an efficient infections segmentation with minimal human intervention. Our contributions include 1) a conditional image synthesis network that receives both style information from reference CT images and structural information from unsupervised segmentation masks, and 2) a corresponding segmentation mask synthesis network to automatically segment these synthesized images simultaneously. Our experimental studies on HRCT scans collected from COVID-19 patients demonstrate that our CS² model can lead to realistic synthesized datasets and promising segmentation results of COVID infections compared to the state-of-the-art nnUNet trained and fine-tuned in a fully supervised manner.
   </left>
  </p>
  <br/>
  <hr/>
  <center>
   <h2>
    Method
   </h2>
  </center>
  <p id="method" style="text-align:justify; text-justify:inter-ideograph;">
   <left>
    The novelty of our work is three-fold: 1) we develop a novel unsupervised mask-to-image synthesis pipeline that generates images controllably without human labeling; 2) instead of directly using the numeric and disarranged unsupervised segmentation masks, which are cluttered with over-segmented super-pixels, we assign the mean Hounsfield unit (HU) value for each cluster in the unsupervised segmentation masks to obtain an ordered and well-organized labeling; and 3) we propose a new synthesis network structure featured by multiple adaptive instance normalization (AdaIN) blocks that handles unaligned structural and tissue information.
   </left>
  </p>
  <table align="center" id="graph abstract" width="800px">
   <tbody>
    <tr>
     <td align="center" width="600px">
      <center>
       <img src="./resources/fig1.png" style="width:650px"/>
      </center>
     </td>
    </tr>
   </tbody>
  </table>
  <br/>
  <hr/>
  <center>
   <h2 id="results">
    Results
   </h2>
   <p align="left">
    <strong>
     Example synthetic images from four generative models.
    </strong>
   </p>
   <table align="center" width="800px">
    <tbody>
     <tr>
      <td align="center" width="800px">
       <center>
        <img src="./resources/fig4.png" style="width:600px"/>
       </center>
      </td>
     </tr>
    </tbody>
   </table>
   <p align="left">
    The masks are synthetic segmentation masks that corresponded with these images. Red pixels indicate lung tissues and green pixels are GGOs. We also include synthetic samples of different modalities in our supplementary file.
   </p>
   <p align="left">
    <strong>
     The synthetic images of our model are structurally editable
    </strong>
   </p>
   <table align="center" width="800px">
    <tbody>
     <tr>
      <td align="center" width="800px">
       <center>
        <img src="./resources/fig6.png" style="width:600px"/>
       </center>
      </td>
     </tr>
    </tbody>
   </table>
   <p align="left">
    An example of our synthetic images (b) structurally edited with the Unsupervised masks (a) by adding circular patches of different HU values (1-4) and radii (5-8). The patches in (a1) to (a4) have a radius of 30 pixels, and the patches in (a1) to (a4) have a mean HU value of -600
   </p>
   <p align="left">
    <strong>
     Applications on brain MRI
    </strong>
   </p>
   <table align="center" width="800px">
    <tbody>
     <tr>
      <td align="center" width="800px">
       <center>
        <img src="./resources/figS8.png" style="width:600px"/>
       </center>
      </td>
     </tr>
    </tbody>
   </table>
   <p align="left">
    Our V2M2I model can synthesize four modalities at the same time, as well as their segmentation results. Fig. 5 is a typical failed cases with scattering wrongly segmented pixels with 8% wrongly segmented pixels. Post-processing algorithms such as connected component analysis, can remove these scatters.
   </p>
   <p align="left">
    <strong>
     Applications on lung fibrosis HRCT
    </strong>
   </p>
   <table align="center" width="800px">
    <tbody>
     <tr>
      <td align="center" width="800px">
       <center>
        <img src="./resources/figS7.png" style="width:600px"/>
       </center>
      </td>
     </tr>
    </tbody>
   </table>
   <p align="left">
    The HRCT appearance of IPF is more complex than GGO and consolidation. We trained a synthesizer on UIP cases only, and inferenced the UIP synthesizer with non-UIP HRCT images. As is shown, by improving the class number of unsupervised guidance maps, we can successfully model the appearance of IPF as well as controllably synthesize IPF CT images.
   </p>
  </center>
  <center>
   <h2>
    Acknowledgements
   </h2>
  </center>
  <p>
   The HTML source code is based on a template by
   <a href="http://web.mit.edu/phillipi/">
    Phillip Isola
   </a>
   ,
   <a href="http://richzhang.github.io/">
    Richard Zhang
   </a>
   and
   <a href="https://github.com/zhaoziheng/Website">
    Websites for papers
   </a>
   .
  </p>
  <br/>
  <br/>
 </body>
</html>
